{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Change Data Capture (patterns & demos) \u00b6","title":"Change Data Capture (patterns &amp; demos)"},{"location":"index.html#change-data-capture-patterns-demos","text":"","title":"Change Data Capture (patterns &amp; demos)"},{"location":"cdc-based-example.html","text":"CDC Based Integration Example \u00b6 source: https://github.com/maarten-vandeperre/cdc-based-integration-example/tree/main This project will showcase CDC (i.e., change data capture) integration patterns on OpenShift, making use of Kafka, Debezium, Camel and Knative. Data outline \u00b6 The initial data set consists of a general schema, containing generic identifiers and three tenant schemas, all containing a contracts, people and addresses table (and a many-to-many link table in between people and addresses). Identifiers \u00b6 1 select * from public . identifiers ; Contracts data \u00b6 1 2 3 4 5 6 7 8 9 10 select tenant_database , code , type , name , owner from ( select 'tenant_1' as tenant_database , c . * from tenant_1 . contracts c union select 'tenant_2' as tenant_database , c . * from tenant_2 . contracts c union select 'tenant_3' as tenant_database , c . * from tenant_3 . contracts c ) as tmp order by tenant_database , code ; People data \u00b6 1 2 3 4 5 6 7 8 9 10 select tenant_database , code , first_name , last_name , gender , status from ( select 'tenant_1' as tenant_database , c . * from tenant_1 . people c union select 'tenant_2' as tenant_database , c . * from tenant_2 . people c union select 'tenant_3' as tenant_database , c . * from tenant_3 . people c ) as tmp order by tenant_database , code ; Address data \u00b6 1 2 3 4 5 6 7 8 9 10 select tenant_database , code , type , address_line_1 , address_line_2 , country from ( select 'tenant_1' as tenant_database , c . * from tenant_1 . addresses c union select 'tenant_2' as tenant_database , c . * from tenant_2 . addresses c union select 'tenant_3' as tenant_database , c . * from tenant_3 . addresses c ) as tmp order by tenant_database , code ; Integration (flows) \u00b6 1. Debezium - Kafka - Aggregation in one code file \u00b6 In this flow, we listen with Debezium for changes in contracts tables (all 3 tenants). Whenever a change occurs, a change message is put on a Kafka topic. A Camel integration is listening on this topic and enriches the data with people data (i.e., exposed through another Camel integration). The enriched data is put on a second Kafka topic. A third Camel integration is listening on this enriched data topic and stores the data in an aggregation database (i.e., MongoDB). The integrations are tenant agnostic, but can extract a tenant identifier to authenticate against the appropriate schemas. Flow setup !! Be aware that you need to change to your base url. 1. 1 2 3 4 kamel run src/main/java/demo/integrations/aggregationflow/flow1/PeopleServiceRouteCamelK.java \\ --property postgres-service = integration-database.demo-project.svc.cluster.local ; kamel log people-service-route-camel-k The following curl command should now return data: 1 2 curl \\ --location 'http://people-service-route-camel-k-demo-project.apps.cluster-475kf.475kf.sandbox268.opentlc.com/people/urn:person:t1:0001' 2. ```shell kamel run src/main/java/demo/integrations/aggregationflow/flow1/EnrichContractsRouteCamelK.java \\ --property kafka.bootstrap.servers=my-cluster-kafka-bootstrap.demo-project.svc.cluster.local:9092 \\ --property people-camel-endpoint= http://people-service-route-camel-k-demo-project.apps.cluster-475kf.475kf.sandbox268.opentlc.com ; 1 kamel log enrich-contracts-route-camel-k 3. shell kamel run src/main/java/demo/integrations/aggregationflow/flow1/MongoStoreRouteCamelK.java \\ --property kafka.bootstrap.servers=my-cluster-kafka-bootstrap.demo-project.svc.cluster.local:9092 \\ --property mongo-connection-url=\"mongodb://mongo:mongo@aggregation-database.demo-project.svc.cluster.local:27017/?authSource=admin\" kamel kamel log mongo-store-route-camel-k 4. Running the following update on the postgres database should result in: sql update tenant_1.contracts set name = 'Lease Agreement - updated' where code = 'urn:contract:t1:1'; update tenant_2.contracts set name = 'Lease Agreement - updated 2' where code = 'urn:contract:t2:1'; update tenant_3.contracts set name = 'Lease Agreement - updated' where code = 'urn:contract:t3:1'; 1. A message on the Kafka topic for CDC: shell oc exec -it my-cluster-kafka-0 \\ -- bin/kafka-console-consumer.sh \\ --bootstrap-server my-cluster-kafka-bootstrap.demo-project.svc.cluster.local:9092 \\ --topic legacydatachanged.tenant_2.contracts 2. A message on the Kafka topic for enrichment: shell oc exec -it my-cluster-kafka-0 \\ -- bin/kafka-console-consumer.sh \\ --bootstrap-server my-cluster-kafka-bootstrap.demo-project.svc.cluster.local:9092 \\ --topic enriched_data 3. A new document in the MongoDB database 'aggregation-database'. 2. Debezium - Kafka - Aggregation in one code file (with keeping aggregation data in sync) \u00b6 This flow is an extension on 1. Debezium - Kafka - Aggregation in one code file : The first flow was not complete as the enriched person data was not synced when it changed in the master database. This is solved in this integration flow, by adding a second Debezium connector. 3. Debezium - Kafka - Aggregation in one code file (with calling customer's APIs) \u00b6 This flow is an extension on 1. Debezium - Kafka - Aggregation in one code file (with keeping aggregation data in sync) : The second flow stopped on our generic data model (i.e., the aggregate), but this data needs to be pushed (on change) to our customers in their specific format, which is implemented in this flow. Installation/Configuration \u00b6 TODO, manifest files can be found in this folder and a summier description can be found in commands.md","title":"CDC integration example"},{"location":"cdc-based-example.html#cdc-based-integration-example","text":"source: https://github.com/maarten-vandeperre/cdc-based-integration-example/tree/main This project will showcase CDC (i.e., change data capture) integration patterns on OpenShift, making use of Kafka, Debezium, Camel and Knative.","title":"CDC Based Integration Example"},{"location":"cdc-based-example.html#data-outline","text":"The initial data set consists of a general schema, containing generic identifiers and three tenant schemas, all containing a contracts, people and addresses table (and a many-to-many link table in between people and addresses).","title":"Data outline"},{"location":"cdc-based-example.html#identifiers","text":"1 select * from public . identifiers ;","title":"Identifiers"},{"location":"cdc-based-example.html#contracts-data","text":"1 2 3 4 5 6 7 8 9 10 select tenant_database , code , type , name , owner from ( select 'tenant_1' as tenant_database , c . * from tenant_1 . contracts c union select 'tenant_2' as tenant_database , c . * from tenant_2 . contracts c union select 'tenant_3' as tenant_database , c . * from tenant_3 . contracts c ) as tmp order by tenant_database , code ;","title":"Contracts data"},{"location":"cdc-based-example.html#people-data","text":"1 2 3 4 5 6 7 8 9 10 select tenant_database , code , first_name , last_name , gender , status from ( select 'tenant_1' as tenant_database , c . * from tenant_1 . people c union select 'tenant_2' as tenant_database , c . * from tenant_2 . people c union select 'tenant_3' as tenant_database , c . * from tenant_3 . people c ) as tmp order by tenant_database , code ;","title":"People data"},{"location":"cdc-based-example.html#address-data","text":"1 2 3 4 5 6 7 8 9 10 select tenant_database , code , type , address_line_1 , address_line_2 , country from ( select 'tenant_1' as tenant_database , c . * from tenant_1 . addresses c union select 'tenant_2' as tenant_database , c . * from tenant_2 . addresses c union select 'tenant_3' as tenant_database , c . * from tenant_3 . addresses c ) as tmp order by tenant_database , code ;","title":"Address data"},{"location":"cdc-based-example.html#integration-flows","text":"","title":"Integration (flows)"},{"location":"cdc-based-example.html#1-debezium-kafka-aggregation-in-one-code-file","text":"In this flow, we listen with Debezium for changes in contracts tables (all 3 tenants). Whenever a change occurs, a change message is put on a Kafka topic. A Camel integration is listening on this topic and enriches the data with people data (i.e., exposed through another Camel integration). The enriched data is put on a second Kafka topic. A third Camel integration is listening on this enriched data topic and stores the data in an aggregation database (i.e., MongoDB). The integrations are tenant agnostic, but can extract a tenant identifier to authenticate against the appropriate schemas. Flow setup !! Be aware that you need to change to your base url. 1. 1 2 3 4 kamel run src/main/java/demo/integrations/aggregationflow/flow1/PeopleServiceRouteCamelK.java \\ --property postgres-service = integration-database.demo-project.svc.cluster.local ; kamel log people-service-route-camel-k The following curl command should now return data: 1 2 curl \\ --location 'http://people-service-route-camel-k-demo-project.apps.cluster-475kf.475kf.sandbox268.opentlc.com/people/urn:person:t1:0001' 2. ```shell kamel run src/main/java/demo/integrations/aggregationflow/flow1/EnrichContractsRouteCamelK.java \\ --property kafka.bootstrap.servers=my-cluster-kafka-bootstrap.demo-project.svc.cluster.local:9092 \\ --property people-camel-endpoint= http://people-service-route-camel-k-demo-project.apps.cluster-475kf.475kf.sandbox268.opentlc.com ; 1 kamel log enrich-contracts-route-camel-k 3. shell kamel run src/main/java/demo/integrations/aggregationflow/flow1/MongoStoreRouteCamelK.java \\ --property kafka.bootstrap.servers=my-cluster-kafka-bootstrap.demo-project.svc.cluster.local:9092 \\ --property mongo-connection-url=\"mongodb://mongo:mongo@aggregation-database.demo-project.svc.cluster.local:27017/?authSource=admin\" kamel kamel log mongo-store-route-camel-k 4. Running the following update on the postgres database should result in: sql update tenant_1.contracts set name = 'Lease Agreement - updated' where code = 'urn:contract:t1:1'; update tenant_2.contracts set name = 'Lease Agreement - updated 2' where code = 'urn:contract:t2:1'; update tenant_3.contracts set name = 'Lease Agreement - updated' where code = 'urn:contract:t3:1'; 1. A message on the Kafka topic for CDC: shell oc exec -it my-cluster-kafka-0 \\ -- bin/kafka-console-consumer.sh \\ --bootstrap-server my-cluster-kafka-bootstrap.demo-project.svc.cluster.local:9092 \\ --topic legacydatachanged.tenant_2.contracts 2. A message on the Kafka topic for enrichment: shell oc exec -it my-cluster-kafka-0 \\ -- bin/kafka-console-consumer.sh \\ --bootstrap-server my-cluster-kafka-bootstrap.demo-project.svc.cluster.local:9092 \\ --topic enriched_data 3. A new document in the MongoDB database 'aggregation-database'.","title":"1. Debezium - Kafka - Aggregation in one code file"},{"location":"cdc-based-example.html#2-debezium-kafka-aggregation-in-one-code-file-with-keeping-aggregation-data-in-sync","text":"This flow is an extension on 1. Debezium - Kafka - Aggregation in one code file : The first flow was not complete as the enriched person data was not synced when it changed in the master database. This is solved in this integration flow, by adding a second Debezium connector.","title":"2. Debezium - Kafka - Aggregation in one code file (with keeping aggregation data in sync)"},{"location":"cdc-based-example.html#3-debezium-kafka-aggregation-in-one-code-file-with-calling-customers-apis","text":"This flow is an extension on 1. Debezium - Kafka - Aggregation in one code file (with keeping aggregation data in sync) : The second flow stopped on our generic data model (i.e., the aggregate), but this data needs to be pushed (on change) to our customers in their specific format, which is implemented in this flow.","title":"3. Debezium - Kafka - Aggregation in one code file (with calling customer's APIs)"},{"location":"cdc-based-example.html#installationconfiguration","text":"TODO, manifest files can be found in this folder and a summier description can be found in commands.md","title":"Installation/Configuration"},{"location":"commands.html","text":"","title":"Commands"},{"location":"decompose_the_monolith.html","text":"Decompose the monolith \u00b6 source: https://github.com/maarten-vandeperre/knative-serverless-example-workshop/tree/main/tutorial 1. Outline \u00b6 1.1. Initial setup: the monolith \u00b6 1.2. Containerize the monolith \u00b6 1.3. Extract account service \u00b6 1.4. Extract address service \u00b6 1.5. Extract person service \u00b6 1.6. Kill the monolith \u00b6 2. Step by step execution \u00b6 !!! All following commands should be executed from within the dev spaces workspace in the root of the project !!! 2.1. Create Postgres database \u00b6 1 2 3 4 5 6 7 oc new-app \\ -e POSTGRES_USER = postgres \\ -e POSTGRES_PASSWORD = postgres \\ -e POSTGRES_DB = knative_demo \\ -e PGDATA = /tmp/data/pgdata \\ quay.io/appdev_playground/wal_postgres:0.0.2 \\ --name postgres And add initial data ( !!! replace the pod name in the following example with the pod name of the Postgres pod. You can find it with the oc get pod command ). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 execute them one by one! oc exec -it postgres-7b5478878b-tr9hw -- mkdir /tmp/init-scripts oc rsync ./db-init-scripts/postgres postgres-7b5478878b-tr9hw:/tmp/init-scripts oc exec -it postgres-7b5478878b-tr9hw -- psql -U postgres -d knative_demo -a -f /tmp/init-scripts/postgres/001_setup_addresses_table.sql oc exec -it postgres-7b5478878b-tr9hw -- psql -U postgres -d knative_demo -a -f /tmp/init-scripts/postgres/002_setup_person_table.sql oc exec -it postgres-7b5478878b-tr9hw -- psql -U postgres -d knative_demo -a -f /tmp/init-scripts/postgres/003_add_outbox_tables.sql or ( execute them one by one! ) : oc exec -it $( oc get pod -o custom-columns = POD:.metadata.name --no-headers | grep postgres ) -- mkdir /tmp/init-scripts oc rsync ./db-init-scripts/postgres $( oc get pod -o custom-columns = POD:.metadata.name --no-headers | grep postgres ) :/tmp/init-scripts oc exec -it $( oc get pod -o custom-columns = POD:.metadata.name --no-headers | grep postgres ) -- psql -U postgres -d knative_demo -a -f /tmp/init-scripts/postgres/001_setup_addresses_table.sql oc exec -it $( oc get pod -o custom-columns = POD:.metadata.name --no-headers | grep postgres ) -- psql -U postgres -d knative_demo -a -f /tmp/init-scripts/postgres/002_setup_person_table.sql oc exec -it $( oc get pod -o custom-columns = POD:.metadata.name --no-headers | grep postgres ) -- psql -U postgres -d knative_demo -a -f /tmp/init-scripts/postgres/003_add_outbox_tables.sql 2.2. Create MongoDB database \u00b6 1 2 3 4 5 oc new-app \\ -e MONGO_INITDB_ROOT_USERNAME = mongo \\ -e MONGO_INITDB_ROOT_PASSWORD = mongo \\ mongo:4.2.24 \\ --name knative-mongo 2.3. Deploy the monolith with basic deployment configuration \u00b6 1 sh tutorial/scripts/02_script.sh In order to validate if it ran successfully, you can check the output of the monolith route 2.4. Deploy the monolith with OpenShift Serverless - serving \u00b6 1 sh tutorial/scripts/03_script.sh In order to validate if it ran successfully, you can check the output of the monolith serving route 2.5. Deploy the account microservice with OpenShift Serverless - serving & Source to sink config \u00b6 Within this step, we will extract the account logic (i.e., account microservice) from the monolith. Whenever changes are happening on the monolith, Debezium will detect them and add them to a Kafka topic. There will be a source to sink configuration in place, which will trigger an account microservice data sync when such a message is put on the topic. 1 2 sh tutorial/scripts/04a_script.sh sh tutorial/scripts/04b_script.sh In order to validate if it ran successfully, run following validation checks 1. Check response from account service 2. Open a Kafka consumer to check if messages are generated when the monolith's person or address data changes (i.e., insert, update or delete). !!! Be aware, if you have a different project name, you will have to change it in the bootstrap server url 1 2 3 4 oc exec -it my-cluster-kafka-0 \\ -- bin/kafka-console-consumer.sh \\ --bootstrap-server my-cluster-kafka-bootstrap.demo-project.svc.cluster.local:9092 \\ --topic monolith_data_changed.public.people_changed 3. Execute a CURL call to add a person to the monolith: !!! Be aware, if you have a different project name, you will have to change it in the bootstrap server url 1 2 3 4 5 6 7 curl --location 'https://knative-serving-monolith-demo-project.apps.cluster-l2nk9.l2nk9.sandbox1488.opentlc.com/api/people' \\ --header 'Content-Type: application/json' \\ --header 'Cookie: 1d146a18013fc0b7dc188e8aab4d8b2e=4cba66d56c609e5f7167bd40296c17aa' \\ --data-raw '{ \"firstName\": \"Maarten @ Knative Demo\", \"lastName\": \"Vandeperre\" }' 4. Quickly after the curl command, do a couple of oc get pod commands until you see a pod for the account service up-and-running (as it is serverless, it will be up-and-running for limited amount of time): 5. The consumer should receive a message, referring to the change. 2.6. Deploy the address microservice with OpenShift Serverless - serving & Channel config \u00b6 Within this step, we will extract the address logic (i.e., address microservice) from the monolith. Whenever changes are happening on this microservice, the account microservice will be synced via the serverless channel-subscription topology. You can check the code: WithChannelUpdateAddressRepository 1 sh tutorial/scripts/05_script.sh In order to validate if it ran successfully, run following validation checks 1. Check response from the address microservice 2. Create an address in the address microservice and keep the address UUID (we will link it to a person in a later stage). !!! Be aware, if you have a different project name, you will have to change it in the bootstrap server url 1 2 3 4 5 6 7 8 curl --location 'https://knative-service-microservice-address-demo-project.apps.cluster-l2nk9.l2nk9.sandbox1488.opentlc.com/api/addresses' \\ --header 'Content-Type: application/json' \\ --header 'Cookie: 69c9933e3ba539b1af7eb4375bdc36d0=abfe0f526bf00bb9f8436fdcbd23d641' \\ --data '{ \"addressLine1\": \"Kinepolis\", \"addressLine2\": \"Antwerp\", \"countryIsoCode\": \"BE\" }' (Don't forget to store the resulting UUID, \"4f1ce413-ea9a-47eb-8e4a-ad81c89a2fed\", in our case => see screenshot below). 3. Quickly after the curl command, do a couple of oc get pod commands until you see a pod for the account service up-and-running (as it is serverless, it will be up-and-running for limited amount of time): 2.7. Deploy the person microservice with OpenShift Serverless - serving & Trigger - broker config \u00b6 Within this step, we will extract the person logic (i.e., person microservice) from the monolith. Whenever changes are happening on this microservice, the account microservice will be synced via the serverless trigger-broker topology. You can check the code: WithChannelUpdatePersonRepository 1 sh tutorial/scripts/06_script.sh In order to validate if it ran successfully, run following validation checks 1. Check response from the person microservice 2. Create a person in the person microservice and use the address UUID from previous section. !!! Be aware, if you have a different project name, you will have to change it in the bootstrap server url 1 2 3 4 5 6 7 8 curl --location 'https://knative-service-microservice-person-demo-project.apps.cluster-l2nk9.l2nk9.sandbox1488.opentlc.com/api/people' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"firstName\": \"Maarten @ Knative Demo - With address\", \"lastName\": \"Vandeperre\", \"birthDate\": \"17/04/1989\", \"addressRef\": \"4f1ce413-ea9a-47eb-8e4a-ad81c89a2fed\" }' 3. Quickly after the curl command, do a couple of oc get pod commands until you see a pod for the account service up-and-running (as it is serverless, it will be up-and-running for limited amount of time): 4. Now that we have a person entity linked to an address, we should be able to see the corresponding account entity: 1 curl https://knative-service-microservice-account-demo-project.apps.cluster-l2nk9.l2nk9.sandbox1488.opentlc.com/api/accounts | jq 2.8. Kill the monolith!! \u00b6 Debug \u00b6 1 oc get pod 1 oc port-forward knative-mongo-c9b4cf7f-pg5fw 27017 :27017 Showcase Debezium \u00b6 1 2 3 4 5 6 7 8 curl --location 'https://knative-serving-monolith-demo-project.apps.cluster-8f4q9.8f4q9.sandbox1055.opentlc.com/api/people' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"firstName\": \"Maarten @ Monolith\", \"lastName\": \"Vandeperre\", \"birthDate\": \"17/04/1989\", \"addressRef\": \"4f1ce413-ea9a-47eb-8e4a-ad81c89a2fed\" }' Check Mongo: person not in the person microservice database, but it is in the account person database. Hence, the CDC-pipeline worked.","title":"Decompose the monolith with CDC"},{"location":"decompose_the_monolith.html#decompose-the-monolith","text":"source: https://github.com/maarten-vandeperre/knative-serverless-example-workshop/tree/main/tutorial","title":"Decompose the monolith"},{"location":"decompose_the_monolith.html#1-outline","text":"","title":"1. Outline"},{"location":"decompose_the_monolith.html#11-initial-setup-the-monolith","text":"","title":"1.1. Initial setup: the monolith"},{"location":"decompose_the_monolith.html#12-containerize-the-monolith","text":"","title":"1.2. Containerize the monolith"},{"location":"decompose_the_monolith.html#13-extract-account-service","text":"","title":"1.3. Extract account service"},{"location":"decompose_the_monolith.html#14-extract-address-service","text":"","title":"1.4. Extract address service"},{"location":"decompose_the_monolith.html#15-extract-person-service","text":"","title":"1.5. Extract person service"},{"location":"decompose_the_monolith.html#16-kill-the-monolith","text":"","title":"1.6. Kill the monolith"},{"location":"decompose_the_monolith.html#2-step-by-step-execution","text":"!!! All following commands should be executed from within the dev spaces workspace in the root of the project !!!","title":"2. Step by step execution"},{"location":"decompose_the_monolith.html#21-create-postgres-database","text":"1 2 3 4 5 6 7 oc new-app \\ -e POSTGRES_USER = postgres \\ -e POSTGRES_PASSWORD = postgres \\ -e POSTGRES_DB = knative_demo \\ -e PGDATA = /tmp/data/pgdata \\ quay.io/appdev_playground/wal_postgres:0.0.2 \\ --name postgres And add initial data ( !!! replace the pod name in the following example with the pod name of the Postgres pod. You can find it with the oc get pod command ). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 execute them one by one! oc exec -it postgres-7b5478878b-tr9hw -- mkdir /tmp/init-scripts oc rsync ./db-init-scripts/postgres postgres-7b5478878b-tr9hw:/tmp/init-scripts oc exec -it postgres-7b5478878b-tr9hw -- psql -U postgres -d knative_demo -a -f /tmp/init-scripts/postgres/001_setup_addresses_table.sql oc exec -it postgres-7b5478878b-tr9hw -- psql -U postgres -d knative_demo -a -f /tmp/init-scripts/postgres/002_setup_person_table.sql oc exec -it postgres-7b5478878b-tr9hw -- psql -U postgres -d knative_demo -a -f /tmp/init-scripts/postgres/003_add_outbox_tables.sql or ( execute them one by one! ) : oc exec -it $( oc get pod -o custom-columns = POD:.metadata.name --no-headers | grep postgres ) -- mkdir /tmp/init-scripts oc rsync ./db-init-scripts/postgres $( oc get pod -o custom-columns = POD:.metadata.name --no-headers | grep postgres ) :/tmp/init-scripts oc exec -it $( oc get pod -o custom-columns = POD:.metadata.name --no-headers | grep postgres ) -- psql -U postgres -d knative_demo -a -f /tmp/init-scripts/postgres/001_setup_addresses_table.sql oc exec -it $( oc get pod -o custom-columns = POD:.metadata.name --no-headers | grep postgres ) -- psql -U postgres -d knative_demo -a -f /tmp/init-scripts/postgres/002_setup_person_table.sql oc exec -it $( oc get pod -o custom-columns = POD:.metadata.name --no-headers | grep postgres ) -- psql -U postgres -d knative_demo -a -f /tmp/init-scripts/postgres/003_add_outbox_tables.sql","title":"2.1. Create Postgres database"},{"location":"decompose_the_monolith.html#22-create-mongodb-database","text":"1 2 3 4 5 oc new-app \\ -e MONGO_INITDB_ROOT_USERNAME = mongo \\ -e MONGO_INITDB_ROOT_PASSWORD = mongo \\ mongo:4.2.24 \\ --name knative-mongo","title":"2.2. Create MongoDB database"},{"location":"decompose_the_monolith.html#23-deploy-the-monolith-with-basic-deployment-configuration","text":"1 sh tutorial/scripts/02_script.sh In order to validate if it ran successfully, you can check the output of the monolith route","title":"2.3. Deploy the monolith with basic deployment configuration"},{"location":"decompose_the_monolith.html#24-deploy-the-monolith-with-openshift-serverless-serving","text":"1 sh tutorial/scripts/03_script.sh In order to validate if it ran successfully, you can check the output of the monolith serving route","title":"2.4. Deploy the monolith with OpenShift Serverless - serving"},{"location":"decompose_the_monolith.html#25-deploy-the-account-microservice-with-openshift-serverless-serving-source-to-sink-config","text":"Within this step, we will extract the account logic (i.e., account microservice) from the monolith. Whenever changes are happening on the monolith, Debezium will detect them and add them to a Kafka topic. There will be a source to sink configuration in place, which will trigger an account microservice data sync when such a message is put on the topic. 1 2 sh tutorial/scripts/04a_script.sh sh tutorial/scripts/04b_script.sh In order to validate if it ran successfully, run following validation checks 1. Check response from account service 2. Open a Kafka consumer to check if messages are generated when the monolith's person or address data changes (i.e., insert, update or delete). !!! Be aware, if you have a different project name, you will have to change it in the bootstrap server url 1 2 3 4 oc exec -it my-cluster-kafka-0 \\ -- bin/kafka-console-consumer.sh \\ --bootstrap-server my-cluster-kafka-bootstrap.demo-project.svc.cluster.local:9092 \\ --topic monolith_data_changed.public.people_changed 3. Execute a CURL call to add a person to the monolith: !!! Be aware, if you have a different project name, you will have to change it in the bootstrap server url 1 2 3 4 5 6 7 curl --location 'https://knative-serving-monolith-demo-project.apps.cluster-l2nk9.l2nk9.sandbox1488.opentlc.com/api/people' \\ --header 'Content-Type: application/json' \\ --header 'Cookie: 1d146a18013fc0b7dc188e8aab4d8b2e=4cba66d56c609e5f7167bd40296c17aa' \\ --data-raw '{ \"firstName\": \"Maarten @ Knative Demo\", \"lastName\": \"Vandeperre\" }' 4. Quickly after the curl command, do a couple of oc get pod commands until you see a pod for the account service up-and-running (as it is serverless, it will be up-and-running for limited amount of time): 5. The consumer should receive a message, referring to the change.","title":"2.5. Deploy the account microservice with OpenShift Serverless - serving &amp; Source to sink config"},{"location":"decompose_the_monolith.html#26-deploy-the-address-microservice-with-openshift-serverless-serving-channel-config","text":"Within this step, we will extract the address logic (i.e., address microservice) from the monolith. Whenever changes are happening on this microservice, the account microservice will be synced via the serverless channel-subscription topology. You can check the code: WithChannelUpdateAddressRepository 1 sh tutorial/scripts/05_script.sh In order to validate if it ran successfully, run following validation checks 1. Check response from the address microservice 2. Create an address in the address microservice and keep the address UUID (we will link it to a person in a later stage). !!! Be aware, if you have a different project name, you will have to change it in the bootstrap server url 1 2 3 4 5 6 7 8 curl --location 'https://knative-service-microservice-address-demo-project.apps.cluster-l2nk9.l2nk9.sandbox1488.opentlc.com/api/addresses' \\ --header 'Content-Type: application/json' \\ --header 'Cookie: 69c9933e3ba539b1af7eb4375bdc36d0=abfe0f526bf00bb9f8436fdcbd23d641' \\ --data '{ \"addressLine1\": \"Kinepolis\", \"addressLine2\": \"Antwerp\", \"countryIsoCode\": \"BE\" }' (Don't forget to store the resulting UUID, \"4f1ce413-ea9a-47eb-8e4a-ad81c89a2fed\", in our case => see screenshot below). 3. Quickly after the curl command, do a couple of oc get pod commands until you see a pod for the account service up-and-running (as it is serverless, it will be up-and-running for limited amount of time):","title":"2.6. Deploy the address microservice with OpenShift Serverless - serving &amp; Channel config"},{"location":"decompose_the_monolith.html#27-deploy-the-person-microservice-with-openshift-serverless-serving-trigger-broker-config","text":"Within this step, we will extract the person logic (i.e., person microservice) from the monolith. Whenever changes are happening on this microservice, the account microservice will be synced via the serverless trigger-broker topology. You can check the code: WithChannelUpdatePersonRepository 1 sh tutorial/scripts/06_script.sh In order to validate if it ran successfully, run following validation checks 1. Check response from the person microservice 2. Create a person in the person microservice and use the address UUID from previous section. !!! Be aware, if you have a different project name, you will have to change it in the bootstrap server url 1 2 3 4 5 6 7 8 curl --location 'https://knative-service-microservice-person-demo-project.apps.cluster-l2nk9.l2nk9.sandbox1488.opentlc.com/api/people' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"firstName\": \"Maarten @ Knative Demo - With address\", \"lastName\": \"Vandeperre\", \"birthDate\": \"17/04/1989\", \"addressRef\": \"4f1ce413-ea9a-47eb-8e4a-ad81c89a2fed\" }' 3. Quickly after the curl command, do a couple of oc get pod commands until you see a pod for the account service up-and-running (as it is serverless, it will be up-and-running for limited amount of time): 4. Now that we have a person entity linked to an address, we should be able to see the corresponding account entity: 1 curl https://knative-service-microservice-account-demo-project.apps.cluster-l2nk9.l2nk9.sandbox1488.opentlc.com/api/accounts | jq","title":"2.7. Deploy the person microservice with OpenShift Serverless - serving &amp; Trigger - broker config"},{"location":"decompose_the_monolith.html#28-kill-the-monolith","text":"","title":"2.8. Kill the monolith!!"},{"location":"decompose_the_monolith.html#debug","text":"1 oc get pod 1 oc port-forward knative-mongo-c9b4cf7f-pg5fw 27017 :27017","title":"Debug"},{"location":"decompose_the_monolith.html#showcase-debezium","text":"1 2 3 4 5 6 7 8 curl --location 'https://knative-serving-monolith-demo-project.apps.cluster-8f4q9.8f4q9.sandbox1055.opentlc.com/api/people' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"firstName\": \"Maarten @ Monolith\", \"lastName\": \"Vandeperre\", \"birthDate\": \"17/04/1989\", \"addressRef\": \"4f1ce413-ea9a-47eb-8e4a-ad81c89a2fed\" }' Check Mongo: person not in the person microservice database, but it is in the account person database. Hence, the CDC-pipeline worked.","title":"Showcase Debezium"},{"location":"install_and_configure_the_operators.html","text":"Install and Configure the operators \u00b6 source: https://github.com/maarten-vandeperre/knative-serverless-example-workshop/tree/main/tutorial AMQ Streams - Kafka Dev spaces OpenShift serverless - Knative AMQ Streams - Install Operator \u00b6 Step 1 Step 2 Step 3 (Keep default configurations) Wait until the resources are provisioned and the operator is in a ready state (this can take some minutes). Dev Spaces - Install Operator \u00b6 Step 1 Step 2 Step 3 (Keep default configurations) OpenShift Serverless (Knative) - Install Operator \u00b6 Step 1 Step 2 Step 3 (Keep default configurations)","title":"Install operators"},{"location":"install_and_configure_the_operators.html#install-and-configure-the-operators","text":"source: https://github.com/maarten-vandeperre/knative-serverless-example-workshop/tree/main/tutorial AMQ Streams - Kafka Dev spaces OpenShift serverless - Knative","title":"Install and Configure the operators"},{"location":"install_and_configure_the_operators.html#amq-streams-install-operator","text":"Step 1 Step 2 Step 3 (Keep default configurations) Wait until the resources are provisioned and the operator is in a ready state (this can take some minutes).","title":"AMQ Streams - Install Operator"},{"location":"install_and_configure_the_operators.html#dev-spaces-install-operator","text":"Step 1 Step 2 Step 3 (Keep default configurations)","title":"Dev Spaces - Install Operator"},{"location":"install_and_configure_the_operators.html#openshift-serverless-knative-install-operator","text":"Step 1 Step 2 Step 3 (Keep default configurations)","title":"OpenShift Serverless (Knative) - Install Operator"},{"location":"wrap_up_operator_config.html","text":"Wrap up operator config \u00b6 source: https://github.com/maarten-vandeperre/knative-serverless-example-workshop/tree/main/tutorial Dev spaces Metadata configuration AMQ Streams - Kafka OpenShift serverless - Knative Dev spaces \u00b6 Open specifications tab. Click \"create Che cluster\": A cloud development environment (CDE) service for OpenShift. Built on the open source Eclipse Che project, Red Hat OpenShift Dev Spaces uses Kubernetes and containers to provide developers and other IT team members with a consistent, secure, and zero-configuration development environment. Create Che cluster with default settings. Select the route for your OpenShift dev space (it can take some minutes for this to pop up). Or Click \"Login with OpenShift\". Fill in login credentials. Allow the selected permissions. Link the current workshop GitHub repository ( https://github.com/maarten-vandeperre/knative-serverless-example-workshop ). Wait for the provisioning of the workspace (can take some minutes as well). Open a terminal and start exploring. E.g., 1 oc whoami 1 oc get pod Metadata configuration \u00b6 Make sure you're using the created project 1 oc project demo-project Change the content of the tutorial/scripts/.namespace file (in the dev spaces workspace) to your project name. For us, it is \"demo-project\" Change the content of the tutorial/scripts/.root_domain file (in the dev spaces workspace) to your base domain from the OpenShift sandbox. For us, it is \"apps.cluster-gq27g.gq27g.sandbox3037.opentlc.com\" Do a find and replace on the domain Fetch a Quay.io CLI secret in order to execute docker push commands to your Quay repository (will be required for debezium). 1. Go to quay.io. 2. Go to user settings. 3. Click \"generate encrypted password\" 4. Go to Docker config and view the password. 5. Store the encrypted password in rh-ee-mvandepe-auth.json file in the root of the project. (i.e., replace with it). 6. Store configuration in a secret 1 2 3 4 oc create secret generic \\ kafka-connect-cluster-push-secret \\ --from-file = .dockerconfigjson = ./rh-ee-mvandepe-auth.json \\ --type = kubernetes.io/dockerconfigjson AMQ Streams - Kafka \u00b6 Open Kafka tab. Click \"create Kafka\" (in the current namespace). Click \"create Kafka with default settings\". Check for resources to be provisioned. When all resources are available, the cluster status should reach the ready state. Create the Kafka connect cluster (Debezium connector): !!! Make sure that you have configured the metadata as described in previous section !!! Execute the following command in the terminal of the dev space, in the project root: 1 sh tutorial/scripts/01_script.sh Created Kafka Connect cluster should be visible in the UI. OpenShift serverless - Knative \u00b6 Open \"Knative Eventing\" tab. Click \"create Knative eventing\". !!! This should be created in the \"knative-eventing\" namespace !!! Open \"Knative Serving\" tab. Click \"create Knative serving\". !!! This should be created in the \"knative-serving\" namespace !!! Create with default values. When all resources are provisioned, you should be able to see the serverless section for all namespaces. We'll check for our \"demo-project\" namespace. Open \"Knative Kafka\" tab. Create the Knative Kafka cluster. !!! list all the brokers (from the namespaces) you want to use for broker and channel and enable source and sink. If you followed our namings, it should be: 1 my-cluster-kafka-bootstrap.demo-project.svc.cluster.local:9092","title":"Configure platform"},{"location":"wrap_up_operator_config.html#wrap-up-operator-config","text":"source: https://github.com/maarten-vandeperre/knative-serverless-example-workshop/tree/main/tutorial Dev spaces Metadata configuration AMQ Streams - Kafka OpenShift serverless - Knative","title":"Wrap up operator config"},{"location":"wrap_up_operator_config.html#dev-spaces","text":"Open specifications tab. Click \"create Che cluster\": A cloud development environment (CDE) service for OpenShift. Built on the open source Eclipse Che project, Red Hat OpenShift Dev Spaces uses Kubernetes and containers to provide developers and other IT team members with a consistent, secure, and zero-configuration development environment. Create Che cluster with default settings. Select the route for your OpenShift dev space (it can take some minutes for this to pop up). Or Click \"Login with OpenShift\". Fill in login credentials. Allow the selected permissions. Link the current workshop GitHub repository ( https://github.com/maarten-vandeperre/knative-serverless-example-workshop ). Wait for the provisioning of the workspace (can take some minutes as well). Open a terminal and start exploring. E.g., 1 oc whoami 1 oc get pod","title":"Dev spaces"},{"location":"wrap_up_operator_config.html#metadata-configuration","text":"Make sure you're using the created project 1 oc project demo-project Change the content of the tutorial/scripts/.namespace file (in the dev spaces workspace) to your project name. For us, it is \"demo-project\" Change the content of the tutorial/scripts/.root_domain file (in the dev spaces workspace) to your base domain from the OpenShift sandbox. For us, it is \"apps.cluster-gq27g.gq27g.sandbox3037.opentlc.com\" Do a find and replace on the domain Fetch a Quay.io CLI secret in order to execute docker push commands to your Quay repository (will be required for debezium). 1. Go to quay.io. 2. Go to user settings. 3. Click \"generate encrypted password\" 4. Go to Docker config and view the password. 5. Store the encrypted password in rh-ee-mvandepe-auth.json file in the root of the project. (i.e., replace with it). 6. Store configuration in a secret 1 2 3 4 oc create secret generic \\ kafka-connect-cluster-push-secret \\ --from-file = .dockerconfigjson = ./rh-ee-mvandepe-auth.json \\ --type = kubernetes.io/dockerconfigjson","title":"Metadata configuration"},{"location":"wrap_up_operator_config.html#amq-streams-kafka","text":"Open Kafka tab. Click \"create Kafka\" (in the current namespace). Click \"create Kafka with default settings\". Check for resources to be provisioned. When all resources are available, the cluster status should reach the ready state. Create the Kafka connect cluster (Debezium connector): !!! Make sure that you have configured the metadata as described in previous section !!! Execute the following command in the terminal of the dev space, in the project root: 1 sh tutorial/scripts/01_script.sh Created Kafka Connect cluster should be visible in the UI.","title":"AMQ Streams - Kafka"},{"location":"wrap_up_operator_config.html#openshift-serverless-knative","text":"Open \"Knative Eventing\" tab. Click \"create Knative eventing\". !!! This should be created in the \"knative-eventing\" namespace !!! Open \"Knative Serving\" tab. Click \"create Knative serving\". !!! This should be created in the \"knative-serving\" namespace !!! Create with default values. When all resources are provisioned, you should be able to see the serverless section for all namespaces. We'll check for our \"demo-project\" namespace. Open \"Knative Kafka\" tab. Create the Knative Kafka cluster. !!! list all the brokers (from the namespaces) you want to use for broker and channel and enable source and sink. If you followed our namings, it should be: 1 my-cluster-kafka-bootstrap.demo-project.svc.cluster.local:9092","title":"OpenShift serverless - Knative"}]}